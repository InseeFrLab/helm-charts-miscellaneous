# Default values for vllm chart.

global:
  suspend: false

llm-serving:
  nameOverride: ""
  fullnameOverride: "llm-serving"
  podAnnotations: {}

  service:
    image:
      version: "vllm/vllm-openai:v0.8.2"
      pullPolicy: IfNotPresent
      custom:
        enabled: false
        version: ""

  resources: {}

  huggingFace: 
    hfToken: ""

  llm:
    hfModelName: meta-llama/Llama-3.2-1B-Instruct
    localPath: "/tmp/.cache/huggingface"
    args: '--gpu-memory-utilization "0.95" --dtype "auto" --max-model-len "8200"'

  networking:
    port:
      number: 8000

  ingress:
      enabled: true
      className: ""
      annotations: 
        nginx.ingress.kubernetes.io/proxy-read-timeout: "3600"
      hostname: "chart-example.local"

  s3:
    enabled: false # Set to true to use S3
    s3ModelPath: ""
    accessKeyId: ""
    endpoint: ""
    defaultRegion: ""
    secretAccessKey: ""
    sessionToken: ""

  nodeSelector: {}
